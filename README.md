This project presents an integrated system aimed at enhancing the accessibility and usability of audio
content through source audio separation techniques. Source audio separation
involves the decomposition of a complex mixture of audio streams into its individual constituents,
while captioning involves the generation of textual transcriptions of the audio content. Accurately
separating individual audio sources from an acoustic mixture and captioning it accordingly to solve the
problem of subtitle mismatch during speech overlap is the main objective of our research. The
proposed system leverages state-of-the-art machine learning algorithms with signal processing
techniques and NLP methodologies to achieve accurate subtitling. Our proposed system showcases
potential to significantly enhance the accessibility and utility of audio content across diverse domains
as well as real-world scenarios.
